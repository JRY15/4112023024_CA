import re

class Lexer:
    def __init__(self, source_code):
        self.source_code = source_code
        self.tokens = []
        self.keywords = ['if', 'else', 'while', 'for', 'int', 'float', 'print']
        self.token_regex = {
            'INTEGER': r'^[0-9]+$',
            'FLOAT': r'^[0-9]+\.[0-9]+$',
            'IDENTIFIER': r'^[a-zA-Z_][a-zA-Z0-9_]*$',
            'OPERATOR': r'^[+\-*/]$',
            'PUNCTUATION': r'^[(){}]+$',
            'ASSIGNMENT': r'^=$'
        }

    def tokenize(self):
        source_code = self.source_code.split()

        for word in source_code:
            token_type = self.get_token_type(word)
            if token_type is not None:
                if token_type == 'INTEGER':
                    self.tokens.append((token_type, int(word)))
                elif token_type == 'FLOAT':
                    self.tokens.append((token_type, float(word)))
                else:
                    self.tokens.append((token_type, word))
            else:
                self.tokens.append(('UNKNOWN', word))

    def get_token_type(self, word):
        for token_type, pattern in self.token_regex.items():
            if re.match(pattern, word):
                return token_type
        if word in self.keywords:
            return 'KEYWORD'
        return None

    def display_tokens(self):
        for token in self.tokens:
            print(token)

source_code = """
int main() {
    int x = 10;
    float y = 3.14;
    if (x > 5) {
        print("Hello, world!");
    } else {
        print("Goodbye, world!");
    }
    return 0;
}
"""

lexer = Lexer(source_code)
lexer.tokenize()
lexer.display_tokens()
